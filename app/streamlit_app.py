"""Streamlit UI ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒª"""
import streamlit as st
from streamlit_autorefresh import st_autorefresh
import pandas as pd
import json
import os
import re
from datetime import datetime, timedelta
from pathlib import Path

st.set_page_config(
    page_title="AIåˆ†æã‚¢ãƒ—ãƒª",
    page_icon="ğŸ“Š",
    layout="wide",
)

# === ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦– ===

PARAMS_FILE = Path("input/params.json")

def load_params_from_file():
    """å¤–éƒ¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    if not PARAMS_FILE.exists():
        return None, None, [], None
    try:
        mtime = PARAMS_FILE.stat().st_mtime
        with open(PARAMS_FILE, "r", encoding="utf-8") as f:
            raw_params = json.load(f)
        canonical = canonicalize_json(raw_params)
        params, errors = validate_params(raw_params)
        return params, mtime, errors, canonical
    except json.JSONDecodeError as e:
        return None, None, [{
            "error_code": "INVALID_JSON",
            "message": f"Invalid JSON: {e}",
            "path": "$",
            "hint": "Fix JSON syntax in input/params.json."
        }], None
    except IOError as e:
        return None, None, [{
            "error_code": "FILE_IO_ERROR",
            "message": f"Failed to read params.json: {e}",
            "path": "$",
            "hint": "Check file permissions and file path."
        }], None

def apply_params_to_session(params):
    """èª­ã¿è¾¼ã‚“ã ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«åæ˜ ï¼ˆã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã®keyã‚‚æ›´æ–°ï¼‰"""
    if params is None:
        return False

    st.session_state["loaded_params"] = params
    st.session_state["params_applied"] = True

    # ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã®keyã‚’ç›´æ¥æ›´æ–°ï¼ˆã“ã‚Œã§UIã«å³åº§ã«åæ˜ ã•ã‚Œã‚‹ï¼‰
    source = params.get("source", "ga4").lower()

    # æ—¥ä»˜
    date_range = params.get("date_range", {})
    if date_range.get("start"):
        st.session_state["w_start_date"] = datetime.strptime(date_range["start"], "%Y-%m-%d").date()
    if date_range.get("end"):
        st.session_state["w_end_date"] = datetime.strptime(date_range["end"], "%Y-%m-%d").date()

    # å–å¾—ä»¶æ•°
    if "limit" in params:
        st.session_state["w_limit"] = params["limit"]

    # ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³
    if "dimensions" in params:
        if source == "gsc":
            st.session_state["w_gsc_dimensions"] = params["dimensions"]
        else:
            st.session_state["w_ga4_dimensions"] = params["dimensions"]

    # GA4å›ºæœ‰
    if source == "ga4":
        if "property_id" in params:
            st.session_state["w_ga4_property_id"] = params["property_id"]
        if "metrics" in params:
            st.session_state["w_ga4_metrics"] = params["metrics"]
        if "filter_d" in params:
            st.session_state["w_ga4_filter"] = params["filter_d"]

    # GSCå›ºæœ‰
    if source == "gsc":
        if "site_url" in params:
            st.session_state["w_gsc_site"] = params["site_url"]
        if "filter" in params:
            st.session_state["w_gsc_filter"] = params["filter"]

    # BigQueryå›ºæœ‰
    if source == "bigquery":
        if "project_id" in params:
            st.session_state["w_bq_project"] = params["project_id"]
        if "sql" in params:
            st.session_state["w_bq_sql"] = params["sql"]

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆæœªæŒ‡å®šé …ç›®ã‚’æ®‹ã•ãªã„ã‚ˆã†æ¯å›åˆæœŸåŒ–ã—ã¦ã‹ã‚‰åæ˜ ï¼‰
    pipeline = params.get("pipeline") or {}
    st.session_state["w_tf_date"] = False
    st.session_state["w_tf_url_decode"] = False
    st.session_state["w_tf_strip_qs"] = False
    st.session_state["w_tf_keep_params"] = ""
    st.session_state["w_tf_path_only"] = False
    st.session_state["w_pipeline_where"] = ""
    st.session_state["w_pipeline_columns"] = []
    st.session_state["w_pipeline_group_by"] = []
    st.session_state["w_pipeline_head"] = 0
    for key in list(st.session_state.keys()):
        if key.startswith("w_agg_"):
            del st.session_state[key]

    if pipeline:
        if pipeline.get("transform"):
            expr = pipeline["transform"]
            try:
                transforms = parse_transforms(expr)
            except ValueError:
                transforms = []
            for _, func, args in transforms:
                if func == "date_format":
                    st.session_state["w_tf_date"] = True
                elif func == "url_decode":
                    st.session_state["w_tf_url_decode"] = True
                elif func == "strip_qs":
                    st.session_state["w_tf_strip_qs"] = True
                    if args:
                        st.session_state["w_tf_keep_params"] = args
                elif func == "path_only":
                    st.session_state["w_tf_path_only"] = True
        if pipeline.get("where"):
            st.session_state["w_pipeline_where"] = pipeline["where"]
        if pipeline.get("columns"):
            st.session_state["w_pipeline_columns"] = [
                c.strip() for c in pipeline["columns"].split(",") if c.strip()
            ]
        if pipeline.get("group_by"):
            st.session_state["w_pipeline_group_by"] = [
                c.strip() for c in pipeline["group_by"].split(",") if c.strip()
            ]
        if pipeline.get("aggregate"):
            for part in pipeline["aggregate"].split(","):
                tokens = [x.strip() for x in part.split(":", 1)]
                if len(tokens) == 2 and tokens[0] and tokens[1]:
                    func, col = tokens
                    st.session_state[f"w_agg_{col}"] = func
        if pipeline.get("head") is not None:
            st.session_state["w_pipeline_head"] = pipeline["head"]

    # save
    save = params.get("save") or {}
    # åˆæœŸåŒ–
    st.session_state["w_sheet_url"] = ""
    st.session_state["w_sheet_name"] = "data"
    st.session_state["w_save_mode"] = "ä¸Šæ›¸ã"
    st.session_state["w_save_bq_project"] = ""
    st.session_state["w_save_bq_dataset"] = ""
    st.session_state["w_save_bq_table"] = ""
    st.session_state["w_save_bq_mode"] = "ä¸Šæ›¸ã"
    st.session_state["w_save_filename"] = ""

    if save:
        mode_rmap = {"overwrite": "ä¸Šæ›¸ã", "append": "è¿½è¨˜", "upsert": "ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ"}
        target = save.get("to")

        if target == "csv":
            path = save.get("path", "")
            if path:
                st.session_state["w_save_filename"] = Path(path).name

        elif target == "sheets":
            st.session_state["w_sheet_url"] = save.get("sheet_url", "")
            st.session_state["w_sheet_name"] = save.get("sheet_name", "data")
            st.session_state["w_save_mode"] = mode_rmap.get(save.get("mode", "overwrite"), "ä¸Šæ›¸ã")
            if save.get("keys"):
                st.session_state["w_upsert_keys"] = save["keys"]

        elif target == "bigquery":
            st.session_state["w_save_bq_project"] = save.get("project_id", "")
            st.session_state["w_save_bq_dataset"] = save.get("dataset", "")
            st.session_state["w_save_bq_table"] = save.get("table", "")
            st.session_state["w_save_bq_mode"] = mode_rmap.get(save.get("mode", "overwrite"), "ä¸Šæ›¸ã")

    return True

def check_file_updated():
    """ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆmtime + å®Ÿè³ªå·®åˆ†ï¼‰"""
    if not PARAMS_FILE.exists():
        return False, None, None, []

    current_mtime = PARAMS_FILE.stat().st_mtime
    last_mtime = st.session_state.get("last_params_mtime", 0)

    if current_mtime <= last_mtime:
        return False, None, None, []

    params, mtime, errors, canonical = load_params_from_file()
    st.session_state["last_params_mtime"] = current_mtime

    last_canonical = st.session_state.get("last_params_canonical")
    if not has_effective_params_update(current_mtime, last_mtime, canonical, last_canonical):
        return False, None, None, []

    st.session_state["last_params_canonical"] = canonical
    return True, params, mtime, errors

# === å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ===
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from lib.megaton_client import (
    get_megaton,
    get_ga4_properties as _get_ga4_properties,
    query_ga4,
    get_gsc_sites as _get_gsc_sites,
    query_gsc,
    get_bq_datasets as _get_bq_datasets,
    query_bq,
    save_to_sheet,
    save_to_bq,
)
from lib.params_diff import canonicalize_json
from lib.params_validator import validate_params
from lib.result_inspector import apply_pipeline, SUPPORTED_AGG_FUNCS, parse_transforms
from app.ui.params_utils import (
    GA4_OPERATORS,
    GSC_OPERATORS,
    parse_ga4_filter_to_df,
    serialize_ga4_filter_from_df,
    parse_gsc_filter_to_df,
    serialize_gsc_filter_from_df,
    has_effective_params_update,
)

# Streamlitç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ©ãƒƒãƒ‘ãƒ¼
@st.cache_data(ttl=300)
def get_ga4_properties():
    return _get_ga4_properties()

@st.cache_data(ttl=300)
def get_gsc_sites():
    return _get_gsc_sites()

@st.cache_data(ttl=300)
def get_bq_datasets(project_id):
    return _get_bq_datasets(project_id)

@st.cache_data(ttl=60)
def execute_ga4_query(property_id, start_date, end_date, dimensions, metrics, filter_d, limit):
    return query_ga4(property_id, start_date, end_date, dimensions, metrics, filter_d, limit)

@st.cache_data(ttl=60)
def execute_gsc_query(site_url, start_date, end_date, dimensions, limit, dimension_filter=None):
    return query_gsc(site_url, start_date, end_date, dimensions, limit, dimension_filter)

def parse_gsc_filter(filter_str: str):
    """GSCãƒ•ã‚£ãƒ«ã‚¿æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆAPIç”¨ï¼‰"""
    if not filter_str or not filter_str.strip():
        return None
    filters = []
    for part in filter_str.split(";"):
        parts = part.split(":", 2)
        if len(parts) == 3:
            filters.append({
                "dimension": parts[0],
                "operator": parts[1],
                "expression": parts[2]
            })
    return filters if filters else None


def execute_bq_query(project_id, sql):
    return query_bq(project_id, sql)


# === UI ===

st.title("ğŸ“Š AIåˆ†æã‚¢ãƒ—ãƒª")

# === ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ===

# ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
if "auto_watch" not in st.session_state:
    st.session_state["auto_watch"] = True
if "auto_execute" not in st.session_state:
    st.session_state["auto_execute"] = False
if "params_validation_errors" not in st.session_state:
    st.session_state["params_validation_errors"] = []
if "last_params_canonical" not in st.session_state:
    st.session_state["last_params_canonical"] = None

# è‡ªå‹•ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ç”¨ï¼š2ç§’ã”ã¨ï¼‰
if st.session_state.get("auto_watch", True):
    st_autorefresh(interval=2000, limit=None, key="file_watcher_refresh")

# ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ãƒã‚§ãƒƒã‚¯ç”¨ãƒ•ãƒ©ã‚°
file_just_updated = False

# ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã§å®Ÿè¡Œï¼‰
if st.session_state.get("auto_watch", True):
    updated, params, _, errors = check_file_updated()
    if updated:
        if params:
            apply_params_to_session(params)
            st.session_state["params_validation_errors"] = []
            st.toast("ğŸ”„ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸ", icon="ğŸ“„")
            file_just_updated = True
            if st.session_state.get("auto_execute", False):
                st.session_state["auto_execute_pending"] = True
        elif errors:
            st.session_state["params_validation_errors"] = errors
            st.toast("âŒ params.json ã®æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ", icon="âš ï¸")

with st.sidebar:
    with st.expander("ğŸ¤– AI Agent é€£æº", expanded=True):
        st.session_state["auto_watch"] = st.toggle(
            "JSONè‡ªå‹•åæ˜ ",
            value=st.session_state.get("auto_watch", True),
            help="input/params.json ã®å¤‰æ›´ã‚’2ç§’ã”ã¨ã«æ¤œçŸ¥"
        )
        st.session_state["auto_execute"] = st.toggle(
            "è‡ªå‹•å®Ÿè¡Œ",
            value=st.session_state.get("auto_execute", False),
            help="ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¾Œã«è‡ªå‹•ã§ã‚¯ã‚¨ãƒªå®Ÿè¡Œ"
        )

        # ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ…‹è¡¨ç¤º
        if PARAMS_FILE.exists():
            mtime = datetime.fromtimestamp(PARAMS_FILE.stat().st_mtime)
            st.caption(f"ğŸ“„ params.json: {mtime.strftime('%H:%M:%S')} æ›´æ–°")
        else:
            st.caption("ğŸ“„ params.json: ãªã—")

        # æ‰‹å‹•èª­ã¿è¾¼ã¿ãƒœã‚¿ãƒ³
        if st.button("ğŸ“¥ JSONã‚’é–‹ã", width="stretch"):
            params, mtime, errors, canonical = load_params_from_file()
            if params:
                apply_params_to_session(params)
                st.session_state["last_params_mtime"] = mtime
                st.session_state["last_params_canonical"] = canonical
                st.session_state["params_validation_errors"] = []
                st.success("âœ“ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                st.rerun()
            elif errors:
                if canonical is not None:
                    st.session_state["last_params_canonical"] = canonical
                if mtime is not None:
                    st.session_state["last_params_mtime"] = mtime
                st.session_state["params_validation_errors"] = errors
                st.error("params.json ã®æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")
            else:
                st.warning("params.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("è¨­å®š")

    validation_errors = st.session_state.get("params_validation_errors", [])
    if validation_errors:
        st.error("params.json ãŒã‚¹ã‚­ãƒ¼ãƒä¸ä¸€è‡´ã§ã™")
        for err in validation_errors:
            st.caption(f"`{err['error_code']}` {err['path']} - {err['message']}")

    # èª­ã¿è¾¼ã¿æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
    lp = st.session_state.get("loaded_params", {})

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åæ˜ æ™‚ã®é€šçŸ¥
    if st.session_state.get("params_applied"):
        st.info("ğŸ“¥ ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åæ˜ ã—ã¾ã—ãŸ")
        st.session_state["params_applied"] = False

    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠ
    source_map = {"ga4": "GA4", "gsc": "GSC", "bigquery": "BigQuery"}
    default_source = source_map.get(lp.get("source", "ga4").lower(), "GA4")
    source = st.radio("ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹", ["GA4", "GSC", "BigQuery"], horizontal=True,
                      index=["GA4", "GSC", "BigQuery"].index(default_source))

    st.divider()

    # BigQueryä»¥å¤–ã¯æ—¥ä»˜ç¯„å›²ã‚’è¡¨ç¤º
    if source != "BigQuery":
        # æ—¥ä»˜ç¯„å›²ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        if "w_start_date" not in st.session_state:
            date_range = lp.get("date_range", {})
            st.session_state["w_start_date"] = datetime.strptime(date_range["start"], "%Y-%m-%d").date() if date_range.get("start") else (datetime.now() - timedelta(days=14)).date()
            st.session_state["w_end_date"] = datetime.strptime(date_range["end"], "%Y-%m-%d").date() if date_range.get("end") else (datetime.now() - timedelta(days=1)).date()

        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input("é–‹å§‹æ—¥", key="w_start_date")
        with col2:
            end_date = st.date_input("çµ‚äº†æ—¥", key="w_end_date")

        st.divider()

    if source == "GA4":
        # GA4è¨­å®š
        try:
            properties = get_ga4_properties()
        except (RuntimeError, FileNotFoundError, ValueError) as e:
            st.error(f"âš ï¸ èªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            st.stop()
        property_options = {p["display"]: p["id"] for p in properties}

        # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£IDã‹ã‚‰displayåã‚’é€†å¼•ãï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã¾ãŸã¯loaded_paramsã‹ã‚‰ï¼‰
        default_prop_idx = 0
        loaded_prop_id = st.session_state.get("w_ga4_property_id") or lp.get("property_id", "")
        for i, (display, pid) in enumerate(property_options.items()):
            if pid == loaded_prop_id:
                default_prop_idx = i
                break

        selected_property = st.selectbox("ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£", list(property_options.keys()), index=default_prop_idx)
        property_id = property_options[selected_property]

        # ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆåˆæœŸåŒ–ï¼‰
        all_dimensions = ["date", "sessionDefaultChannelGroup", "sessionSource", "sessionMedium",
                         "pagePath", "landingPage", "deviceCategory", "country"]
        if "w_ga4_dimensions" not in st.session_state:
            st.session_state["w_ga4_dimensions"] = lp.get("dimensions", ["date"]) if lp.get("source", "").lower() == "ga4" else ["date"]
        dimensions = st.multiselect("ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³", all_dimensions, key="w_ga4_dimensions")

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆåˆæœŸåŒ–ï¼‰
        all_metrics = ["sessions", "activeUsers", "newUsers", "screenPageViews",
                      "bounceRate", "averageSessionDuration", "conversions"]
        if "w_ga4_metrics" not in st.session_state:
            st.session_state["w_ga4_metrics"] = lp.get("metrics", ["sessions", "activeUsers"]) if lp.get("source", "").lower() == "ga4" else ["sessions", "activeUsers"]
        metrics = st.multiselect("ãƒ¡ãƒˆãƒªã‚¯ã‚¹", all_metrics, key="w_ga4_metrics")

        # ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆåˆæœŸåŒ–ï¼‰
        if "w_ga4_filter" not in st.session_state:
            st.session_state["w_ga4_filter"] = lp.get("filter_d", "") if lp.get("source", "").lower() == "ga4" else ""
        
        # ãƒ•ã‚£ãƒ«ã‚¿ã‚’DataFrameã«ãƒ‘ãƒ¼ã‚¹
        filter_df = parse_ga4_filter_to_df(st.session_state.get("w_ga4_filter", ""))
        
        with st.expander("ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶", expanded=bool(len(filter_df))):
            # ã‚ˆãä½¿ã†ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³
            ga4_filter_dims = ["sessionDefaultChannelGroup", "sessionSource", "sessionMedium", 
                               "pagePath", "landingPage", "deviceCategory", "country", "city"]
            
            edited_filter_df = st.data_editor(
                filter_df,
                column_config={
                    "å¯¾è±¡": st.column_config.SelectboxColumn(
                        "å¯¾è±¡",
                        options=ga4_filter_dims + dimensions,
                        required=True,
                    ),
                    "æ¼”ç®—å­": st.column_config.SelectboxColumn(
                        "æ¼”ç®—å­",
                        options=GA4_OPERATORS,
                        required=True,
                    ),
                    "å€¤": st.column_config.TextColumn("å€¤", required=True),
                },
                num_rows="dynamic",
                width="stretch",
                key="ga4_filter_editor"
            )
            
            # DataFrameã‹ã‚‰æ–‡å­—åˆ—ã«å¤‰æ›
            filter_d = serialize_ga4_filter_from_df(edited_filter_df)
            st.session_state["w_ga4_filter"] = filter_d
            
            if filter_d:
                st.caption(f"ğŸ“ `{filter_d}`")

    elif source == "GSC":
        # GSCè¨­å®š
        try:
            sites = get_gsc_sites()
        except (RuntimeError, FileNotFoundError, ValueError) as e:
            st.error(f"âš ï¸ èªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            st.stop()

        # ã‚µã‚¤ãƒˆURLã®åˆæœŸé¸æŠï¼ˆkeyã‚’ä½¿ã£ã¦åˆ¶å¾¡ï¼‰
        if "w_gsc_site" not in st.session_state:
            loaded_site_url = lp.get("site_url", "")
            if loaded_site_url in sites:
                st.session_state["w_gsc_site"] = loaded_site_url
            elif sites:
                st.session_state["w_gsc_site"] = sites[0]

        site_url = st.selectbox("ã‚µã‚¤ãƒˆ", sites, key="w_gsc_site")

        # ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆåˆæœŸåŒ–ï¼‰
        all_gsc_dims = ["query", "page", "country", "device", "date"]
        if "w_gsc_dimensions" not in st.session_state:
            st.session_state["w_gsc_dimensions"] = lp.get("dimensions", ["query"]) if lp.get("source", "").lower() == "gsc" else ["query"]
        dimensions = st.multiselect("ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³", all_gsc_dims, key="w_gsc_dimensions")
        
        # ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆåˆæœŸåŒ–ï¼‰
        if "w_gsc_filter" not in st.session_state:
            st.session_state["w_gsc_filter"] = lp.get("filter", "") if lp.get("source", "").lower() == "gsc" else ""
        
        # ãƒ•ã‚£ãƒ«ã‚¿ã‚’DataFrameã«ãƒ‘ãƒ¼ã‚¹
        gsc_filter_df = parse_gsc_filter_to_df(st.session_state.get("w_gsc_filter", ""))
        
        with st.expander("ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶", expanded=bool(len(gsc_filter_df))):
            gsc_filter_dims = ["query", "page", "country", "device", "date"]
            
            edited_gsc_filter_df = st.data_editor(
                gsc_filter_df,
                column_config={
                    "å¯¾è±¡": st.column_config.SelectboxColumn(
                        "å¯¾è±¡",
                        options=gsc_filter_dims,
                        required=True,
                    ),
                    "æ¼”ç®—å­": st.column_config.SelectboxColumn(
                        "æ¼”ç®—å­",
                        options=GSC_OPERATORS,
                        required=True,
                    ),
                    "å€¤": st.column_config.TextColumn("å€¤", required=True),
                },
                num_rows="dynamic",
                width="stretch",
                key="gsc_filter_editor"
            )
            
            # DataFrameã‹ã‚‰æ–‡å­—åˆ—ã«å¤‰æ›
            gsc_filter = serialize_gsc_filter_from_df(edited_gsc_filter_df)
            st.session_state["w_gsc_filter"] = gsc_filter
            
            if gsc_filter:
                st.caption(f"ğŸ“ `{gsc_filter}`")

    else:
        # BigQueryè¨­å®š
        if "w_bq_project" not in st.session_state:
            st.session_state["w_bq_project"] = lp.get("project_id", "")
        bq_project = st.text_input("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID", key="w_bq_project")

    # å–å¾—ä»¶æ•°ï¼ˆBigQueryä»¥å¤–ï¼‰
    if source != "BigQuery":
        if "w_limit" not in st.session_state:
            st.session_state["w_limit"] = lp.get("limit", 1000)
        # ã‚«ãƒ³ãƒå½¢å¼ã§é¸æŠè‚¢ã‚’è¡¨ç¤º
        limit_options = [100, 500, 1000, 5000, 10000, 25000, 50000, 100000]
        limit_labels = {v: f"{v:,}" for v in limit_options}
        
        # ç¾åœ¨å€¤ãŒé¸æŠè‚¢ã«ãªã„å ´åˆã¯æœ€ã‚‚è¿‘ã„å€¤ã‚’é¸æŠ
        current_limit = st.session_state.get("w_limit", 1000)
        if current_limit not in limit_options:
            current_limit = min(limit_options, key=lambda x: abs(x - current_limit))
        
        limit = st.select_slider(
            "å–å¾—ä»¶æ•°",
            options=limit_options,
            value=current_limit,
            format_func=lambda x: limit_labels[x],
            key="w_limit"
        )

    st.divider()

    execute_btn = st.button("ğŸš€ å®Ÿè¡Œ", type="primary", width="stretch")

# è‡ªå‹•å®Ÿè¡Œãƒã‚§ãƒƒã‚¯
auto_execute_pending = st.session_state.get("auto_execute_pending", False)
if auto_execute_pending:
    st.session_state["auto_execute_pending"] = False  # ãƒ•ãƒ©ã‚°ã‚’ã‚¯ãƒªã‚¢

# BigQuery SQLå…¥åŠ›ã‚¨ãƒªã‚¢ï¼ˆãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ã«è¡¨ç¤ºï¼‰
if source == "BigQuery":
    st.subheader("SQL ã‚¯ã‚¨ãƒª")
    
    # ã‚µãƒ³ãƒ—ãƒ«SQL
    sample_sql = """SELECT 
    event_date,
    COUNT(*) as event_count
FROM `project.analytics_123456789.events_*`
WHERE _TABLE_SUFFIX BETWEEN '20260101' AND '20260131'
GROUP BY event_date
ORDER BY event_date"""
    
    if "w_bq_sql" not in st.session_state:
        st.session_state["w_bq_sql"] = lp.get("sql", sample_sql) if lp.get("source", "").lower() == "bigquery" else sample_sql
    
    sql = st.text_area("SQL", height=200, key="w_bq_sql")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§è¡¨ç¤º
    if bq_project:
        with st.expander("ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§"):
            try:
                datasets = get_bq_datasets(bq_project)
                if datasets:
                    st.write(", ".join(datasets))
                else:
                    st.info("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            except Exception as e:
                st.warning(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
if execute_btn or auto_execute_pending or (file_just_updated and st.session_state.get("auto_execute", False)):
    with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."):
        try:
            if source == "GA4":
                df = execute_ga4_query(
                    property_id,
                    start_date.strftime("%Y-%m-%d"),
                    end_date.strftime("%Y-%m-%d"),
                    dimensions,
                    metrics,
                    filter_d,
                    limit
                )
            elif source == "GSC":
                gsc_dimension_filter = parse_gsc_filter(gsc_filter) if 'gsc_filter' in dir() else None
                df = execute_gsc_query(
                    site_url,
                    start_date.strftime("%Y-%m-%d"),
                    end_date.strftime("%Y-%m-%d"),
                    dimensions,
                    limit,
                    gsc_dimension_filter
                )
            else:
                # BigQuery
                if not bq_project:
                    st.error("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                    df = None
                elif not sql.strip():
                    st.error("SQLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                    df = None
                else:
                    df = execute_bq_query(bq_project, sql)
            
            if df is not None and not df.empty:
                st.success(f"âœ“ {len(df):,} è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
                st.session_state["df"] = df
            elif df is not None:
                st.warning("ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

# çµæœè¡¨ç¤º
if "df" in st.session_state:
    raw_df = st.session_state["df"]

    # === ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³UI ===
    with st.expander("ğŸ”§ çµæœã®çµã‚Šè¾¼ã¿ãƒ»é›†è¨ˆ", expanded=False):
        pipeline_kwargs = {}

        # --- å¤‰æ› ---
        st.markdown("**å¤‰æ›**")
        transform_parts = []

        has_date_col = "date" in raw_df.columns
        # URLåˆ—ãŒã‚ã‚‹ã‹åˆ¤å®šï¼ˆå€¤ãŒ http ã§å§‹ã¾ã‚‹æ–‡å­—åˆ—åˆ—ã‚’æ¢ã™ï¼‰
        url_cols = []
        for c in raw_df.select_dtypes(include="object").columns:
            sample = raw_df[c].dropna().head(5).astype(str)
            if sample.str.startswith("http").any():
                url_cols.append(c)
        has_url_col = len(url_cols) > 0

        pcol1, pcol2 = st.columns(2)
        with pcol1:
            tf_date = st.checkbox(
                "æ—¥ä»˜ã‚’ YYYY-MM-DD ã«å¤‰æ›",
                disabled=not has_date_col,
                key="w_tf_date",
            )
            if tf_date and has_date_col:
                transform_parts.append("date:date_format")

            tf_url_decode = st.checkbox(
                "URLãƒ‡ã‚³ãƒ¼ãƒ‰",
                disabled=not has_url_col,
                key="w_tf_url_decode",
            )
            if tf_url_decode and url_cols:
                for uc in url_cols:
                    transform_parts.append(f"{uc}:url_decode")

        with pcol2:
            tf_strip_qs = st.checkbox(
                "ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—ã‚’é™¤å»",
                disabled=not has_url_col,
                key="w_tf_strip_qs",
            )
            if tf_strip_qs and url_cols:
                keep_params = st.text_input(
                    "æ®‹ã™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€ç©º=å…¨é™¤å»ï¼‰",
                    key="w_tf_keep_params",
                    placeholder="id,ref",
                )
                for uc in url_cols:
                    if keep_params.strip():
                        transform_parts.append(f"{uc}:strip_qs:{keep_params.strip()}")
                    else:
                        transform_parts.append(f"{uc}:strip_qs")

            tf_path_only = st.checkbox(
                "ãƒ‘ã‚¹ã®ã¿ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³é™¤å»ï¼‰",
                disabled=not has_url_col,
                key="w_tf_path_only",
            )
            if tf_path_only and url_cols:
                for uc in url_cols:
                    transform_parts.append(f"{uc}:path_only")

        if transform_parts:
            pipeline_kwargs["transform"] = ",".join(transform_parts)

        st.divider()

        # --- ãƒ•ã‚£ãƒ«ã‚¿ ---
        st.markdown("**ãƒ•ã‚£ãƒ«ã‚¿**")
        where_expr = st.text_input(
            "æ¡ä»¶å¼ï¼ˆpandas queryæ§‹æ–‡ï¼‰",
            key="w_pipeline_where",
            placeholder='clicks > 100 and page.str.contains("/blog/")',
        )
        if where_expr.strip():
            pipeline_kwargs["where"] = where_expr.strip()

        st.divider()

        # --- è¡¨ç¤ºåˆ— ---
        st.markdown("**è¡¨ç¤ºåˆ—**")
        selected_cols = st.multiselect(
            "åˆ—ã‚’é¸æŠï¼ˆç©º=å…¨åˆ—ï¼‰",
            list(raw_df.columns),
            key="w_pipeline_columns",
        )
        if selected_cols:
            pipeline_kwargs["columns"] = ",".join(selected_cols)

        st.divider()

        # --- ã‚°ãƒ«ãƒ¼ãƒ—é›†è¨ˆ ---
        st.markdown("**ã‚°ãƒ«ãƒ¼ãƒ—é›†è¨ˆ**")
        group_cols = st.multiselect(
            "ã‚°ãƒ«ãƒ¼ãƒ—åˆ—",
            list(raw_df.columns),
            key="w_pipeline_group_by",
        )
        numeric_cols = list(raw_df.select_dtypes(include="number").columns)
        agg_exprs = []
        if group_cols and numeric_cols:
            st.caption("é›†è¨ˆé–¢æ•°ã‚’è¨­å®š")
            for nc in numeric_cols:
                agg_func = st.selectbox(
                    f"{nc}",
                    ["ï¼ˆãªã—ï¼‰", "sum", "mean", "count", "min", "max", "median"],
                    key=f"w_agg_{nc}",
                )
                if agg_func != "ï¼ˆãªã—ï¼‰":
                    agg_exprs.append(f"{agg_func}:{nc}")

        if group_cols and agg_exprs:
            pipeline_kwargs["group_by"] = ",".join(group_cols)
            pipeline_kwargs["aggregate"] = ",".join(agg_exprs)

            # ã‚°ãƒ«ãƒ¼ãƒ—é›†è¨ˆå¾Œã¯ã‚½ãƒ¼ãƒˆåˆ—åãŒå¤‰ã‚ã‚‹ãŸã‚æ›´æ–°
            # sum_clicks ã®ã‚ˆã†ãªåˆ—åã§ã‚½ãƒ¼ãƒˆã—ãŸã„å ´åˆãŒã‚ã‚‹ã®ã§æ¡ˆå†…
            derived_cols = [f"{a.split(':')[0]}_{a.split(':')[1]}" for a in agg_exprs]
            st.caption(f"é›†è¨ˆå¾Œã®åˆ—: {', '.join(group_cols + derived_cols)}")

        st.divider()

        # --- è¡¨ç¤ºè¡Œæ•° ---
        st.markdown("**è¡¨ç¤ºè¡Œæ•°**")
        head_val = st.slider(
            "å…ˆé ­Nè¡Œï¼ˆ0=å…¨è¡Œï¼‰",
            min_value=0,
            max_value=min(len(raw_df), 10000),
            value=0,
            step=10,
            key="w_pipeline_head",
        )
        if head_val > 0:
            pipeline_kwargs["head"] = head_val

    # === ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é©ç”¨ ===
    pipeline_error = None
    if pipeline_kwargs:
        try:
            display_df = apply_pipeline(raw_df, **pipeline_kwargs)
        except ValueError as e:
            pipeline_error = str(e)
            display_df = raw_df
    else:
        display_df = raw_df

    if pipeline_error:
        st.error(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {pipeline_error}")

    # è¡Œæ•°ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³
    if len(display_df) != len(raw_df):
        st.caption(f"ğŸ“Š {len(raw_df):,} è¡Œ â†’ {len(display_df):,} è¡Œ")
    else:
        st.caption(f"ğŸ“Š {len(display_df):,} è¡Œ")

    # ã‚¿ãƒ–
    tab1, tab2, tab3 = st.tabs(["ğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«", "ğŸ“ˆ ãƒãƒ£ãƒ¼ãƒˆ", "ğŸ’¾ ä¿å­˜"])

    with tab1:
        st.dataframe(display_df, width="stretch", height=400)

        # çµ±è¨ˆæƒ…å ±
        with st.expander("çµ±è¨ˆæƒ…å ±"):
            st.write(display_df.describe())

    with tab2:
        if len(display_df.columns) >= 2:
            col1, col2 = st.columns(2)
            with col1:
                x_col = st.selectbox("Xè»¸", display_df.columns)
            with col2:
                y_col = st.selectbox("Yè»¸", [c for c in display_df.columns if c != x_col])

            chart_type = st.radio("ãƒãƒ£ãƒ¼ãƒˆã‚¿ã‚¤ãƒ—", ["æŠ˜ã‚Œç·š", "æ£’ã‚°ãƒ©ãƒ•"], horizontal=True)

            if chart_type == "æŠ˜ã‚Œç·š":
                st.line_chart(display_df.set_index(x_col)[y_col])
            else:
                st.bar_chart(display_df.set_index(x_col)[y_col])

    with tab3:
        st.subheader("ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜")
        save_filename = st.text_input(
            "ãƒ•ã‚¡ã‚¤ãƒ«å",
            value=f"result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            key="w_save_filename",
        )
        col1, col2 = st.columns(2)
        with col1:
            # CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            csv = display_df.to_csv(index=False).encode('utf-8-sig')
            st.download_button(
                "ğŸ“¥ CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                csv,
                save_filename,
                "text/csv",
                width="stretch"
            )
        with col2:
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            if st.button("ğŸ’¾ output/ ã«ä¿å­˜", width="stretch"):
                os.makedirs("output", exist_ok=True)
                filepath = f"output/{save_filename}"
                display_df.to_csv(filepath, index=False, encoding='utf-8-sig')
                st.success(f"ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")

        st.divider()
        st.subheader("Google Sheets ã«ä¿å­˜")

        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆURL
        sheet_url = st.text_input(
            "ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆURL",
            placeholder="https://docs.google.com/spreadsheets/d/xxxxx",
            key="w_sheet_url"
        )

        col1, col2 = st.columns(2)
        with col1:
            sheet_name = st.text_input("ã‚·ãƒ¼ãƒˆå", value="data", key="w_sheet_name")
        with col2:
            save_mode = st.selectbox("ä¿å­˜ãƒ¢ãƒ¼ãƒ‰", ["ä¸Šæ›¸ã", "è¿½è¨˜", "ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ"], key="w_save_mode")

        # ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆæ™‚ã®ã‚­ãƒ¼åˆ—
        if save_mode == "ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ":
            key_cols = st.multiselect("ã‚­ãƒ¼åˆ—", display_df.columns.tolist(), key="w_upsert_keys")

        if st.button("ğŸ“¤ Google Sheets ã«ä¿å­˜", width="stretch", type="primary"):
            if not sheet_url:
                st.error("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆURLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            else:
                try:
                    mode_map = {"ä¸Šæ›¸ã": "overwrite", "è¿½è¨˜": "append", "ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ": "upsert"}
                    mode = mode_map[save_mode]

                    if mode == "upsert" and not key_cols:
                        st.error("ã‚­ãƒ¼åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„")
                    else:
                        save_to_sheet(sheet_url, sheet_name, display_df, mode=mode, keys=key_cols if mode == "upsert" else None)
                        st.success(f"âœ“ ã‚·ãƒ¼ãƒˆã€Œ{sheet_name}ã€ã«ä¿å­˜ã—ã¾ã—ãŸ")
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

        st.divider()
        st.subheader("BigQuery ã«ä¿å­˜")

        bq_project = st.text_input(
            "GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID",
            key="w_save_bq_project",
            placeholder="my-project-id",
        )
        col1, col2 = st.columns(2)
        with col1:
            bq_dataset = st.text_input("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ", key="w_save_bq_dataset")
        with col2:
            bq_table = st.text_input("ãƒ†ãƒ¼ãƒ–ãƒ«", key="w_save_bq_table")

        bq_mode = st.selectbox("ä¿å­˜ãƒ¢ãƒ¼ãƒ‰", ["ä¸Šæ›¸ã", "è¿½è¨˜"], key="w_save_bq_mode")

        if st.button("ğŸ“¤ BigQuery ã«ä¿å­˜", width="stretch", type="primary"):
            if not all([bq_project, bq_dataset, bq_table]):
                st.error("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            else:
                try:
                    bq_mode_map = {"ä¸Šæ›¸ã": "overwrite", "è¿½è¨˜": "append"}
                    save_to_bq(bq_project, bq_dataset, bq_table, display_df, mode=bq_mode_map[bq_mode])
                    st.success(f"âœ“ {bq_project}.{bq_dataset}.{bq_table} ã«ä¿å­˜ã—ã¾ã—ãŸ")
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

# JSONãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¡¨ç¤ºï¼ˆAI Agenté€£æºç”¨ï¼‰
with st.sidebar:
    with st.expander("ğŸ¤– JSON (AI Agentç”¨)"):
        if source == "GA4":
            params = {
                "source": "ga4",
                "property_id": property_id if 'property_id' in dir() else "",
                "date_range": {
                    "start": start_date.strftime("%Y-%m-%d"),
                    "end": end_date.strftime("%Y-%m-%d")
                },
                "dimensions": dimensions if 'dimensions' in dir() else [],
                "metrics": metrics if 'metrics' in dir() else [],
                "filter_d": filter_d if 'filter_d' in dir() else "",
                "limit": limit
            }
        elif source == "GSC":
            params = {
                "source": "gsc",
                "site_url": site_url if 'site_url' in dir() else "",
                "date_range": {
                    "start": start_date.strftime("%Y-%m-%d"),
                    "end": end_date.strftime("%Y-%m-%d")
                },
                "dimensions": dimensions if 'dimensions' in dir() else [],
                "filter": gsc_filter if 'gsc_filter' in dir() else "",
                "limit": limit
            }
        else:
            params = {
                "source": "bigquery",
                "project_id": bq_project if 'bq_project' in dir() else "",
                "sql": sql if 'sql' in dir() else ""
            }
        st.code(json.dumps(params, indent=2, ensure_ascii=False), language="json")
